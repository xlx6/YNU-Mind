{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# RAG \u5b9e\u9a8c\u8bbe\u8ba1\n", "\u672c\u5b9e\u9a8c\u65e8\u5728\u8bc4\u4f30\u4e0d\u540c\u68c0\u7d22\u65b9\u6cd5\uff08BM25\u3001M3E\u3001\u591a\u8def\u53ec\u56de\u3001\u878d\u5408\u53ec\u56de\u3001\u91cd\u6392\u5e8f\uff09\u5728RAG\uff08Retrieval-Augmented Generation\uff09\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\u3002\n", "\u6211\u4eec\u5c06\u6bd4\u8f83 Recall@10, MRR \u548c NDCG@5 \u4e09\u9879\u6307\u6807\u3002"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# \u5b89\u88c5\u5fc5\u8981\u5e93\uff08\u5982\u672a\u5b89\u88c5\uff09\n", "# !pip install rank_bm25 transformers faiss-cpu datasets sklearn\n", "import numpy as np\n", "from sklearn.metrics import ndcg_score\n", "from typing import List\n", "import random\n", "\n", "# \u793a\u4f8b\u6570\u636e\u751f\u6210\n", "queries = [f\"query_{i}\" for i in range(100)]\n", "corpus = [f\"doc_{j}\" for j in range(1000)]\n", "ground_truth = {q: random.sample(corpus, 1) for q in queries}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u5b9a\u4e49\u8bc4\u4f30\u51fd\u6570"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def evaluate_ranking(predictions: List[List[str]], ground_truth: dict, k=10):\n", "    recall_scores, mrr_scores, ndcg_scores = [], [], []\n", "    for i, q in enumerate(queries):\n", "        preds = predictions[i][:k]\n", "        gt = ground_truth[q][0]\n", "\n", "        recall = 1 if gt in preds else 0\n", "        recall_scores.append(recall)\n", "\n", "        if gt in preds:\n", "            rank = preds.index(gt) + 1\n", "            mrr_scores.append(1 / rank)\n", "        else:\n", "            mrr_scores.append(0)\n", "\n", "        relevance = [1 if doc == gt else 0 for doc in preds]\n", "        ndcg_scores.append(ndcg_score([relevance], [list(range(len(relevance), 0, -1))]))\n", "\n", "    return np.mean(recall_scores), np.mean(mrr_scores), np.mean(ndcg_scores)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u6a21\u62df\u4e0d\u540c\u53ec\u56de\u65b9\u6cd5"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# \u7528\u4e8e\u6a21\u62df\u4e0d\u540c\u65b9\u6cd5\u7684\u53ec\u56de\uff08\u7b80\u5355\u968f\u673a\u6a21\u62df\uff09\n", "def simulate_method(hit_ratio=0.7):\n", "    predictions = []\n", "    for q in queries:\n", "        gt = ground_truth[q][0]\n", "        docs = [gt] if random.random() < hit_ratio else []\n", "        docs += random.sample([d for d in corpus if d != gt], 20)\n", "        random.shuffle(docs)\n", "        predictions.append(docs)\n", "    return predictions"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["methods = {\n", "    \"BM25\": 0.68,\n", "    \"M3E\": 0.72,\n", "    \"\u591a\u8def\u53ec\u56de\": 0.84,\n", "    \"\u878d\u5408\u53ec\u56de\": 0.89,\n", "    \"\u91cd\u6392\u5e8f\": 0.93\n", "}\n", "\n", "results = {}\n", "for method, ratio in methods.items():\n", "    preds = simulate_method(ratio)\n", "    recall, mrr, ndcg = evaluate_ranking(preds, ground_truth)\n", "    results[method] = (round(recall, 2), round(mrr, 2), round(ndcg, 2))\n", "\n", "for method, metrics in results.items():\n", "    print(f\"{method}: Recall@10={metrics[0]}, MRR={metrics[1]}, NDCG@5={metrics[2]}\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}