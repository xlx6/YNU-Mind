{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与「中国画学科带头人」最匹配的3位教师：\n",
      "\n",
      "第1名：陈孟昕（相似度：0.8618）\n",
      "职称：\n",
      "简介片段：陈孟昕, 云南大学昌新国际艺术学院中国画学科带头人, 1957年4月生于河北邢台市, 原为湖北美术学...\n",
      "\n",
      "第2名：王林旭（相似度：0.8553）\n",
      "职称：\n",
      "简介片段：王林旭, 云南大学昌新国际艺术学院特聘教授, 国家有突出贡献文化艺术专家, 国务院政府特殊津贴, 人...\n",
      "\n",
      "第3名：张东华（相似度：0.8547）\n",
      "职称：\n",
      "简介片段：张东华, 云南大学昌新国际艺术学院特聘教授、研究生导师, 1967年生于浙江嵊州市, 中国思想与绘画...\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from zhipuai import ZhipuAI\n",
    "import numpy as np\n",
    "\n",
    "# 配置信息（与之前相同）\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"neo4j123456\"\n",
    "\n",
    "class VectorSearcher:\n",
    "    def __init__(self):\n",
    "        self.driver = GraphDatabase.driver(\n",
    "            NEO4J_URI, \n",
    "            auth=(NEO4J_USER, NEO4J_PASSWORD),\n",
    "            encrypted=False\n",
    "        )\n",
    "    \n",
    "    def search_leader(self, query_vector, top_k=3):\n",
    "        cypher = \"\"\"\n",
    "        CALL db.index.vector.queryNodes(\n",
    "            'introduction_vectors',  // 索引名称\n",
    "            $top_k,                  // 返回结果数\n",
    "            $query_vector            // 查询向量\n",
    "        )\n",
    "        YIELD node AS intro, score\n",
    "        MATCH (intro)<-[:HAS_INTRODUCTION]-(t:Faculty)-[:WORKS_IN]->(college)\n",
    "        WHERE college.name = \"昌新国际艺术学院\"\n",
    "        RETURN t.name AS name, \n",
    "            t.position AS position,\n",
    "            intro.content AS intro,\n",
    "            score AS similarity\n",
    "        ORDER BY similarity DESC\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(\n",
    "                    cypher,\n",
    "                    query_vector=query_vector,  # 必须为float列表\n",
    "                    top_k=top_k\n",
    "                )\n",
    "                return [dict(record) for record in result]\n",
    "        except Exception as e:\n",
    "            print(f\"查询失败: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    @staticmethod\n",
    "    def text_to_vector(text):\n",
    "        \"\"\"\n",
    "        文本转向量（需要替换实际的嵌入模型）\n",
    "        :param text: 输入文本\n",
    "        \"\"\"\n",
    "        ZHIPU_API_KEY = \"\"\n",
    "        client = ZhipuAI(api_key=ZHIPU_API_KEY)\n",
    "        response = client.embeddings.create(\n",
    "        model=\"embedding-3\",\n",
    "        dimensions=1024,\n",
    "        input=[text]\n",
    "        )\n",
    "        return np.array(response.data[0].embedding)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    searcher = VectorSearcher()\n",
    "    \n",
    "    # 生成查询向量（示例查询）\n",
    "    query_text = \"中国画学科带头人\"\n",
    "    query_vector = searcher.text_to_vector(query_text)\n",
    "    \n",
    "    # 执行查询\n",
    "    results = searcher.search_leader(query_vector)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(f\"与「{query_text}」最匹配的{len(results)}位教师：\")\n",
    "    for i, res in enumerate(results, 1):\n",
    "        print(f\"\\n第{i}名：{res['name']}（相似度：{res['similarity']:.4f}）\")\n",
    "        print(f\"职称：{res['position']}\")\n",
    "        print(f\"简介片段：{res['intro'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Burning\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "页面文件太小，无法完成操作。 (os error 1455)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 加载分词器（需添加领域词汇）\u001b[39;00m\n\u001b[0;32m     16\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_PATH, local_files_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_PATH, local_files_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# ======== 导出ONNX模型 ========\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport_onnx\u001b[39m():\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# 设置为评估模式\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    572\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    573\u001b[0m     )\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py:4399\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4390\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   4392\u001b[0m     (\n\u001b[0;32m   4393\u001b[0m         model,\n\u001b[0;32m   4394\u001b[0m         missing_keys,\n\u001b[0;32m   4395\u001b[0m         unexpected_keys,\n\u001b[0;32m   4396\u001b[0m         mismatched_keys,\n\u001b[0;32m   4397\u001b[0m         offload_index,\n\u001b[0;32m   4398\u001b[0m         error_msgs,\n\u001b[1;32m-> 4399\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[0;32m   4400\u001b[0m         model,\n\u001b[0;32m   4401\u001b[0m         state_dict,\n\u001b[0;32m   4402\u001b[0m         checkpoint_files,\n\u001b[0;32m   4403\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   4404\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[0;32m   4405\u001b[0m         sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[0;32m   4406\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[0;32m   4407\u001b[0m         disk_offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[0;32m   4408\u001b[0m         offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[0;32m   4409\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[0;32m   4410\u001b[0m         hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[0;32m   4411\u001b[0m         keep_in_fp32_regex\u001b[38;5;241m=\u001b[39mkeep_in_fp32_regex,\n\u001b[0;32m   4412\u001b[0m         device_mesh\u001b[38;5;241m=\u001b[39mdevice_mesh,\n\u001b[0;32m   4413\u001b[0m         key_mapping\u001b[38;5;241m=\u001b[39mkey_mapping,\n\u001b[0;32m   4414\u001b[0m         weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[0;32m   4415\u001b[0m     )\n\u001b[0;32m   4417\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   4418\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py:4638\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[0;32m   4635\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m   4636\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4637\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m-> 4638\u001b[0m         load_state_dict(checkpoint_files[\u001b[38;5;241m0\u001b[39m], map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39mweights_only)\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m   4639\u001b[0m     )\n\u001b[0;32m   4641\u001b[0m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[0;32m   4642\u001b[0m prefix \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbase_model_prefix\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py:513\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03mReads a `safetensor` or a `.bin` checkpoint file. We load the checkpoint on \"cpu\" by default.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_safetensors_available():\n\u001b[1;32m--> 513\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(checkpoint_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    514\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mmetadata()\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlx\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[1;31mOSError\u001b[0m: 页面文件太小，无法完成操作。 (os error 1455)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "# ======== 配置参数 ========\n",
    "MODEL_PATH = \"./intentionmodel/results4english\"  # 模型保存目录\n",
    "ONNX_PATH = \"./intentionmodel/model.onnx\"       # ONNX模型保存路径\n",
    "LABEL_MAP = {0: \"vector_db\", 1: \"neo4j\", 2: \"other\"}\n",
    "MAX_LENGTH = 64  # 与训练时保持一致\n",
    "\n",
    "# ======== 初始化模型和分词器 ========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载分词器（需添加领域词汇）\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, local_files_only=True).to(device)\n",
    "\n",
    "# ======== 导出ONNX模型 ========\n",
    "def export_onnx():\n",
    "    # 设置为评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    # 创建虚拟输入\n",
    "    dummy_text = \"example query\"\n",
    "    inputs = tokenizer(\n",
    "        dummy_text,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 将输入移到对应设备\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # 导出模型\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (input_ids, attention_mask),\n",
    "        ONNX_PATH,\n",
    "        input_names=[\"input_ids\", \"attention_mask\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes={\n",
    "            \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "            \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "            \"logits\": {0: \"batch_size\"}\n",
    "        },\n",
    "        opset_version=13,\n",
    "        do_constant_folding=True\n",
    "    )\n",
    "    print(f\"ONNX模型已导出到：{ONNX_PATH}\")\n",
    "\n",
    "# ======== 预测函数 ========\n",
    "def predict(text):\n",
    "    # 预处理\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # 推理\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 解析结果\n",
    "    probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
    "    pred_label = np.argmax(probs)\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"intent\": LABEL_MAP[pred_label],\n",
    "        \"confidence\": float(probs[pred_label]),\n",
    "        \"details\": {LABEL_MAP[i]: float(probs[i]) for i in range(len(LABEL_MAP))}\n",
    "    }\n",
    "\n",
    "# ======== 交互测试 ========\n",
    "if __name__ == \"__main__\":\n",
    "    # 导出ONNX模型\n",
    "    export_onnx()\n",
    "    \n",
    "    print(\"\\n输入'q'退出测试\")\n",
    "    while True:\n",
    "        text = input(\"\\n请输入问题：\")\n",
    "        if text.lower() == 'q':\n",
    "            break\n",
    "            \n",
    "        result = predict(text)\n",
    "        print(f\"\\n预测结果：{result['intent']}（置信度：{result['confidence']:.2%}）\")\n",
    "        print(\"详细概率：\")\n",
    "        for k, v in result[\"details\"].items():\n",
    "            print(f\"  {k}: {v:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
