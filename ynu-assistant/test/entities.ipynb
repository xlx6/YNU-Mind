{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed55c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "def export_entities():\n",
    "    uri = \"bolt://localhost:7687\"\n",
    "    user = \"neo4j\"\n",
    "    password = \"neo4j123456\"\n",
    "    \n",
    "    with GraphDatabase.driver(uri, auth=(user, password)) as driver:\n",
    "        with driver.session() as session:\n",
    "            # 导出所有教师、课程等实体\n",
    "            result1 = session.run(\"\"\"\n",
    "                MATCH (n:Person)\n",
    "                RETURN n.name\n",
    "            \"\"\")\n",
    "            result2 = session.run(\"\"\"\n",
    "                MATCH (n:Course)\n",
    "                RETURN n.courseName\n",
    "            \"\"\")\n",
    "            result3 = session.run(\"\"\"\n",
    "                MATCH (n:College)\n",
    "                RETURN n.name\n",
    "            \"\"\")\n",
    "\n",
    "            result4 = session.run(\"\"\"\n",
    "                MATCH (n:Classroom)\n",
    "                RETURN n.name\n",
    "            \"\"\")\n",
    "\n",
    "            with open(\"./entities.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                for record in result1:\n",
    "                    f.write(record.value() + \",Person\\n\")\n",
    "                for record in result2:\n",
    "                    if record.value() is not None:\n",
    "                        f.write(record.value() + \",Course\\n\")\n",
    "                for record in result3:\n",
    "                    f.write(record.value() + \",College\\n\")\n",
    "                for record in result4:\n",
    "                    f.write(record.value() + \",Classroom\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "375e62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypinyin import lazy_pinyin, Style\n",
    "\n",
    "with open('../agents/data/entities.txt', 'r', encoding='utf-8') as f:\n",
    "    entities = f.read().split('\\n')\n",
    "\n",
    "pinyin_tntities = []\n",
    "for entity in entities:\n",
    "    if not entity:\n",
    "        continue\n",
    "    tmp = entity.split(',')\n",
    "    assert len(tmp) == 2\n",
    "    py = ''.join(lazy_pinyin(tmp[0], style=Style.NORMAL))\n",
    "    pinyin_tntities.append(py+','+tmp[1])\n",
    "\n",
    "with open('pinyin_entities.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(pinyin_tntities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d66c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entities.txt', 'r', encoding='utf-8') as f:\n",
    "    entities = f.read().split('\\n')\n",
    "with open('entities.txt', 'w', encoding='utf-8') as f:\n",
    "    for entity in entities:\n",
    "        if entity:\n",
    "            f.write(entity + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dd1f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "王金老师教授什么课程。\n",
      "{\n",
      "  \"corrections\": []\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Burning\\AppData\\Roaming\\Python\\Python312\\site-packages\\symspellpy\\symspellpy.py:1137: UserWarning: Failed to parse frequency count as a 64 bit integer.\n",
      "  warnings.warn(\"Failed to parse frequency count as a 64 bit integer.\")\n"
     ]
    }
   ],
   "source": [
    "# spell_checker.py\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import json\n",
    "from typing import Tuple\n",
    "import re\n",
    "\n",
    "class SpellChecker:\n",
    "    def __init__(self):\n",
    "        # 初始化拼写检查器\n",
    "        self.sym_spell = SymSpell(max_dictionary_edit_distance=2)\n",
    "        \n",
    "        # 加载自定义词典（需提前从知识图谱导出实体）\n",
    "        self._load_custom_dictionary(\"./entities.txt\")\n",
    "        \n",
    "        # 加载通用词典\n",
    "        self.sym_spell.load_dictionary('./entities.txt', term_index=0, count_index=1)\n",
    "\n",
    "    def _load_custom_dictionary(self, path: str):\n",
    "        \"\"\"从知识图谱导出实体构建专用词典\"\"\"\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                entity = line.strip()\n",
    "                # 为每个实体设置较高频率（确保优先匹配）\n",
    "                self.sym_spell.create_dictionary_entry(entity, 1000000)\n",
    "\n",
    "    def correct_query(self, query: str) -> Tuple[str, dict]:\n",
    "        \"\"\"\n",
    "        返回纠正后的查询与纠错元数据\n",
    "        \"\"\"\n",
    "        # 分句处理（防止跨句纠错）\n",
    "        sentences = re.split(r'[。！？]', query)\n",
    "        \n",
    "        corrections = []\n",
    "        for sent in sentences:\n",
    "            if not sent:\n",
    "                continue\n",
    "                \n",
    "            # 获取纠错建议\n",
    "            suggestions = self.sym_spell.lookup_compound(\n",
    "                sent, \n",
    "                max_edit_distance=2,\n",
    "                transfer_casing=True\n",
    "            )\n",
    "            \n",
    "            if suggestions:\n",
    "                corrected = suggestions[0].term\n",
    "                if corrected != sent:\n",
    "                    corrections.append({\n",
    "                        \"original\": sent,\n",
    "                        \"corrected\": corrected,\n",
    "                        \"distance\": suggestions[0].distance\n",
    "                    })\n",
    "                sent = corrected\n",
    "        \n",
    "        return '。'.join(sentences), {\"corrections\": corrections}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    checker = SpellChecker()\n",
    "    query = \"王金老师教授什么课程？\"\n",
    "    corrected_query, metadata = checker.correct_query(query)\n",
    "    print(corrected_query)\n",
    "    print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780bb8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Burning\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_group': 'PERSON', 'score': 0.99999905, 'word': '王', 'start': 5, 'end': 6}\n",
      "{'entity_group': 'PERSON', 'score': 0.9999846, 'word': '静', 'start': 6, 'end': 7}\n",
      "{'entity_group': 'FAC', 'score': 0.99999654, 'word': '文 汇', 'start': 10, 'end': 12}\n",
      "{'entity_group': 'FAC', 'score': 0.9999957, 'word': '楼', 'start': 12, 'end': 13}\n",
      "{'entity_group': 'CARDINAL', 'score': 0.99997795, 'word': '3', 'start': 13, 'end': 14}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ckiplab/bert-base-chinese-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"ckiplab/bert-base-chinese-ner\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"请介绍一下王静老师在文汇楼3栋3201上的课程\"\n",
    "\n",
    "entities = nlp(text)\n",
    "for entity in entities:\n",
    "    print(entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13ab6a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请 介绍 一下 王静 老师 在 文汇楼3栋3201 上 的 计算机网络 课程 。\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "# 自定义实体词典\n",
    "jieba.load_userdict(\"./entities.txt\")\n",
    "\n",
    "# 输入文本\n",
    "text = \"请介绍一下王静老师在文汇楼3栋3201上的计算机网络课程。\"\n",
    "\n",
    "# 分词\n",
    "words = jieba.cut(text)\n",
    "\n",
    "# 输出分词结果\n",
    "print(' '.join(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae3510cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "jieba.load_userdict(\"entities.txt\")\n",
    "\n",
    "# 同时将词表存入集合方便快速查询\n",
    "with open(\"entities.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    entity_set = set(line.strip() for line in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "066739ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果: ['王静', '老师', '教授', '的', '课程', '有', '哪些', '？']\n",
      "候选错误词: ['老师', '教授', '课程', '哪些']\n"
     ]
    }
   ],
   "source": [
    "def detect_error_words(text):\n",
    "    words = jieba.lcut(text)\n",
    "    error_candidates = []\n",
    "    for word in words:\n",
    "        if word not in entity_set and len(word) > 1:  # 过滤单字和非实体词\n",
    "            error_candidates.append(word)\n",
    "    return words, error_candidates\n",
    "\n",
    "# 示例\n",
    "text = \"王静老师教授的课程有哪些？\"\n",
    "words, error_candidates = detect_error_words(text)\n",
    "print(\"分词结果:\", words)          # ['我', '打算', '去', '腾训', '公司', '参观', '，', '地址', '在', '南京市', '的', '鼓楼区']\n",
    "print(\"候选错误词:\", error_candidates)  # ['腾训']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee101396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '？']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '？']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '？']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原输入: 王近老师的算法课在哪里上？\n",
      "修正后: 王近老师的算法设计与分析课在哪里上？\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# 实体词表（教师名，课程名，教学楼名，教室名）\n",
    "teachers = [\"王津\", \"李华\", \"赵刚\", \"张丽\"]\n",
    "courses = [\"算法设计与分析\", \"数据结构\", \"人工智能\", \"数据库系统\"]\n",
    "buildings = [\"教学楼A\", \"教学楼B\", \"教学楼C\"]\n",
    "classrooms = [\"101教室\", \"102教室\", \"201教室\", \"202教室\"]\n",
    "\n",
    "# 错别字纠错字典（可以扩展）\n",
    "error_dict = {\n",
    "    \"王金\": \"王津\",  # 错别字\n",
    "    \"算法课\": \"算法设计与分析\",  # 简写\n",
    "}\n",
    "\n",
    "# 使用 fuzzywuzzy 提高匹配准确度\n",
    "def correct_using_fuzzy(input_word, word_list):\n",
    "    # 使用fuzzywuzzy进行模糊匹配，返回匹配度高的实体\n",
    "    matched_word, score = process.extractOne(input_word, word_list, scorer=fuzz.partial_ratio)\n",
    "    # 设置阈值，低于该阈值则认为没有匹配\n",
    "    if score > 80:\n",
    "        return matched_word\n",
    "    return input_word\n",
    "\n",
    "# 实体修正函数，结合 fuzzywuzzy 和字典\n",
    "def correct_entities(text, entity_dict):\n",
    "    words = jieba.cut(text)  # 使用jieba进行分词\n",
    "    corrected_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # 如果存在错误的实体，则进行修正\n",
    "        if word in entity_dict:\n",
    "            corrected_words.append(entity_dict[word])\n",
    "        # 使用 fuzzywuzzy 进行模糊匹配修正\n",
    "        else:\n",
    "            corrected_teacher = correct_using_fuzzy(word, teachers)\n",
    "            corrected_course = correct_using_fuzzy(word, courses)\n",
    "            corrected_building = correct_using_fuzzy(word, buildings)\n",
    "            corrected_classroom = correct_using_fuzzy(word, classrooms)\n",
    "            \n",
    "            # 将修正后的实体添加到结果中\n",
    "            corrected_words.append(corrected_teacher if corrected_teacher != word else\n",
    "                                   corrected_course if corrected_course != word else\n",
    "                                   corrected_building if corrected_building != word else\n",
    "                                   corrected_classroom if corrected_classroom != word else word)\n",
    "\n",
    "    return ''.join(corrected_words)\n",
    "\n",
    "# 示例输入\n",
    "user_input = \"王近老师的算法课在哪里上？\"\n",
    "\n",
    "# 进行实体修正\n",
    "corrected_input = correct_entities(user_input, error_dict)\n",
    "\n",
    "# 输出修正后的结果\n",
    "print(\"原输入:\", user_input)\n",
    "print(\"修正后:\", corrected_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a76fa59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Burning\\AppData\\Local\\Temp\\ipykernel_24024\\451053684.py:35: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(insert_triplets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据插入完成！\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# 连接到Neo4j数据库\n",
    "uri = \"bolt://localhost:7687\"  # Neo4j的地址\n",
    "username = \"neo4j\"  # 用户名\n",
    "password = \"neo4j123456\"  # 密码\n",
    "\n",
    "# 创建Neo4j数据库驱动\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "# 插入数据的函数\n",
    "def insert_triplets(tx):\n",
    "    triplets = [\n",
    "        (\"杨振宁\", \"出生于\", \"中国安徽合肥\"),\n",
    "        (\"杨振宁\", \"毕业于\", \"国立西南联合大学\"),\n",
    "        (\"杨振宁\", \"博士毕业于\", \"芝加哥大学\"),\n",
    "        (\"杨振宁\", \"获奖\", \"诺贝尔物理学奖\"),\n",
    "        (\"杨振宁\", \"研究领域\", \"理论物理\"),\n",
    "        (\"杨振宁\", \"父亲是\", \"杨武之\"),\n",
    "        (\"诺贝尔物理学奖\", \"奖励领域\", \"物理学\"),\n",
    "        (\"杨-米尔斯理论\", \"属于\", \"量子场论领域\"),\n",
    "        (\"杨-米尔斯理论\", \"提出者\", \"杨振宁\"),\n",
    "        (\"杨振宁\", \"合作者\", \"李政道\")\n",
    "    ]\n",
    "    for subj, pred, obj in triplets:\n",
    "        tx.run(\n",
    "            \"MERGE (a:Entity {name: $subj}) \"\n",
    "            \"MERGE (b:Entity {name: $obj}) \"\n",
    "            \"MERGE (a)-[:\" + pred + \"]->(b)\",\n",
    "            subj=subj, obj=obj\n",
    "        )\n",
    "\n",
    "# 插入数据\n",
    "with driver.session() as session:\n",
    "    session.write_transaction(insert_triplets)\n",
    "\n",
    "print(\"数据插入完成！\")\n",
    "\n",
    "# 关闭驱动\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe30ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypinyin import pinyin, lazy_pinyin, Style\n",
    "\n",
    "with open('../agents/data/entities.txt', 'r', encoding='utf-8') as f:\n",
    "    entities = [line.strip() for line in f if line.strip().strip()] \n",
    "p2h = {}\n",
    "entities = set(entities)\n",
    "for entity in entities:\n",
    "    py = ''.join(lazy_pinyin(entity, style=Style.NORMAL))\n",
    "    if py in p2h:\n",
    "        p2h[py].append(entity)\n",
    "    else:\n",
    "        p2h[py] = [entity]\n",
    "for k,v in p2h.items():\n",
    "    if len(v) > 1:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f33a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "def load_entities(entities_path='../agents/data/entities.txt'):\n",
    "        with open(entities_path, 'r', encoding='utf-8') as f:\n",
    "            entities = [line.strip() for line in f if line.strip().strip()]\n",
    "        entities = set(entities)\n",
    "        p2h = {}\n",
    "        for entity in entities:\n",
    "            py = ''.join(lazy_pinyin(entity, style=Style.NORMAL))\n",
    "            if py in p2h:\n",
    "                p2h[py].append(entity)\n",
    "            else:\n",
    "                p2h[py] = [entity]\n",
    "        return entities, p2h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "970028a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "865b3580",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m j \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperson\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: [\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m王静\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassroom\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: [\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m格物楼1栋1507\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcourse\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: [\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m算法课\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m], \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepartment\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: [\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m信院\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]}\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\langchain\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\langchain\\Lib\\json\\decoder.py:338\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    339\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\langchain\\Lib\\json\\decoder.py:354\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 354\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "j = json.loads(\"{'person': ['王静'], 'classroom': ['格物楼1栋1507'], 'course': ['算法课'], 'department': ['信院']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de5c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
